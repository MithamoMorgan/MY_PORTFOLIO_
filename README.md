# My Data Science & Analysis Portfolio

This repository showcases some of my best projects in data analysis and data science, highlighting my ability to extract insights from data and build predictive models. Each project demonstrates my expertise in data manipulation, visualization, and machine learning, applied to solve real-world problems.

Explore these projects to discover the methodologies, code, and analysis that drive actionable insights.

## Diabetes Prediction Project
![](https://github.com/MithamoMorgan/MY_PORTFOLIO_/blob/master/Diabetes.jpg)

**Repository:** [Diabetes Prediction](https://github.com/MithamoMorgan/Diabetes_Prediction)

**Tech Stack:** Python, Streamlit, Scikit-Learn, Pandas, Matplotlib, Seaborn

## Project Overview

This project aims to predict the likelihood of a person developing diabetes based on various medical parameters using a **streamlit app**. It uses historical data to train a machine learning model that can predict diabetes based on factors like age, glucose levels, BMI, HbA1c & others.

The primary focus of this project was to demonstrate how machine learning algorithms (specifically XGBoost) can be used to make predictions in healthcare applications.

## Key Features

* **Data cleaning and preprocessing**
* **Exploratory Data Analysis (EDA)**
* **Model Building**
* **Model Evaluation** 
* **User Input Format**
* **Real-time Prediction**
* **Visualizations**

## Challenges and Learnings

### 1. Data Quality and Preprocessing Challenges:

**Challenge:**

Handling missing or incomplete data, ensuring the dataset was balanced, or dealing with noise in the data.

**Learning:**

I learned how to perform data cleaning and preprocessing techniques like imputation, normalization, and encoding categorical variables. I also learned how to handle class imbalance using techniques such as oversampling, undersampling, and generating synthetic data (e.g., SMOTE).

### 2. Feature Selection and Engineering:

**Challenge:** Deciding which features are most relevant for the prediction of diabetes and how to effectively use them.

**Learning:** I developed skills in feature selection, understanding the impact of different variables on model performance, and creating new features that may help improve accuracy.

### 3. Model Selection and Hyperparameter Tuning:

**Challenge:** Choosing the right model (e.g., logistic regression, decision trees, random forests, or XGBoost) and tuning hyperparameters for optimal performance.

**Learning:** I gained experience in evaluating different models, learning how to use cross-validation and grid search/random search to find the best hyperparameters.

### 4. Dealing with Overfitting or Underfitting:

**Challenge:** Striking the right balance between model complexity and generalization, where models might either memorize the training data (overfitting) or perform poorly (underfitting).

**Learning:** I learned about techniques to mitigate overfitting (e.g., regularization) and methods for improving model performance on unseen data (e.g., cross-validation, ensemble methods).

### 5. Model Evaluation and Metrics:

**Challenge:** Evaluating the model's performance and choosing the appropriate metrics for an imbalanced classification problem (e.g., diabetes prediction often involves an imbalanced dataset where positive cases are fewer than negatives).

**Learning:** I gained insights into metrics beyond accuracy, such as precision, recall, F1-score, and confusion matrices, to better understand model performance in predicting diabetes cases.

### Screenshots


